import subprocess
from glob import glob

# SETTINGS

# Path of the image to be segmented
# (Must be stored as Big Data Viewer XML + HDF5.)
IMAGE = "input/drosophila/image.xml"

# Path of the classifier to be used for segmentation.
# (This should have been created & saved using the Labkit FIJI Plugin.)
CLASSIFIER = "input/drosophila.classifier"

# Number of chunks for the segmentation,
# (This is the number of individual cluster jobs, that will be used for segmentation.)
NUMBER_OF_CHUNKS = 10

# Path to store the resulting Big Data Viewer XML + HDF5
OUTPUT_XML = "output/segmentation.xml"

# Path to temporary folders
N5_FOLDER = "tmp/n5"
SEGMENTATION_PROGRESS = "tmp/progress/segmentation"
HDF5_PROGRESS = "tmp/progress/hdf5"

# Path to the labkit-command-line JAR file.
JAR = glob("labkit-command-line*.jar")[0]

# PROCESSING PREPARATION

# Create temporary n5 dataset
subprocess.check_output(["java", "-jar", JAR, "prepare", "--image", IMAGE, "--classifier", CLASSIFIER, "--n5", N5_FOLDER]);

# Read number of HDF5 partitions. This should equal the number of timepoints
NUMBER_OF_HDF5_PARTITIONS = int(subprocess.check_output(["java", "-jar", JAR, "create-partitioned-hdf5", "--n5", N5_FOLDER, "--xml=dummy.xml", "--number-of-partitions"]))

# SNAKEMAKE RULES

rule all:
	input:
		OUTPUT_XML

rule segment_chunk:
	input:
		IMAGE,
		CLASSIFIER
	output:
		SEGMENTATION_PROGRESS + "/{id}.out"
	shell:
		"java -jar {JAR} segment-chunk --image={IMAGE} --classifier={CLASSIFIER} --n5={N5_FOLDER} --chunks={NUMBER_OF_CHUNKS} --index={wildcards.id} >&{output}"

rule segment_all_chunks:
	input:
		expand("{logs}/{id}.out", logs=SEGMENTATION_PROGRESS, id=range(NUMBER_OF_CHUNKS))
	output:
		SEGMENTATION_PROGRESS + "/done"
	shell:
		"touch {output}"

rule write_output_hdf5:
	input:
 		SEGMENTATION_PROGRESS + "/done"
	output:
		HDF5_PROGRESS + "/{id}.out"
	shell:
		"java -jar {JAR} create-partitioned-hdf5 --n5={N5_FOLDER} --xml={OUTPUT_XML} --partition={wildcards.id} >&{output}"

rule write_output_xml:
	input:
		expand(HDF5_PROGRESS + "/{id}.out", id=range(NUMBER_OF_HDF5_PARTITIONS))
	output:
		OUTPUT_XML
	shell:
		"java -jar {JAR} create-partitioned-hdf5 --n5={N5_FOLDER} --xml={OUTPUT_XML} --header"
